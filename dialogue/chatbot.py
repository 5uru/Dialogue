from langchain.chains import StuffDocumentsChain, LLMChain, ConversationalRetrievalChain
from langchain.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate

EMBEDDING_MODEL_NAME = "thenlper/gte-small"
TEMPLATE = (
    "Combine the chat history and follow up question into "
    "a standalone question. Chat History: {chat_history}"
    "Follow up question: {question}"
)


def get_message(query: str):
    """
     Returns the response from the chatbot based on the given query.

    Args:
        query (str): The query string to be passed to the chatbot.

    Returns:
        str: The response generated by the chatbot.
    """
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vectorstore = Chroma(persist_directory="chroma", embedding_function=embedding)
    llm = Ollama(model="llama2")

    combine_docs_chain = StuffDocumentsChain()
    retriever = vectorstore.as_retriever()

    # This controls how the standalone question is generated.
    # Should take `chat_history` and `question` as input variables.

    prompt = PromptTemplate.from_template(TEMPLATE)
    question_generator_chain = LLMChain(llm=llm, prompt=prompt)
    chain = ConversationalRetrievalChain(
        combine_docs_chain=combine_docs_chain,
        retriever=retriever,
        question_generator=question_generator_chain,
    )
    return chain.query(query)
